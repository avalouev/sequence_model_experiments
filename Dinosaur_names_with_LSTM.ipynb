{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinosaur names with LSTM\n",
    "\n",
    "In this notebook we will a character level LSTM that can be used to generate dinosaur names. This exercise was inspired by an excercise from Andrew Ng's course on Sequence models. I also used the same dataset for training the model.\n",
    "\n",
    "After we train the model using 1536 dinosaur names, we will sample new dinosaur names one letter at a time using our LSTM model. This notebook consists of the following steps:\n",
    "\n",
    "<ul>\n",
    "<li>Implement load_dinosaur_data() function to load dinosaur names from the file</li>\n",
    "<li>Implement preprocess_data() function to encode dinosaur name characters into tensors for training</li>\n",
    "<li>Implement build_model() function to define LSTM model architecture</li>\n",
    "<li>Fit the model using Adam optimizer and cross_entropy loss</li>\n",
    "<li>Implement sample_name_from_model() function to sample new names one letter at a time</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Input, Lambda, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some key parameters. The longest dinosaur name is 27 characters, so let's set the model to 50 characters in case the generate sequence is a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50 # define length of the string to be fed to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data with dinosaur names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dinosaur_data(path):\n",
    "    \"\"\"\n",
    "    Loads and returns dinosaur names data\n",
    "    Arguments:\n",
    "    path -- a string with a path to the file with dinosaur names\n",
    "    \n",
    "    Returns:\n",
    "    names -- a list of dinosaur names\n",
    "    chars -- a list with vocabulary of letters\n",
    "    char_to_idx -- a dictionary mapping characters to their integer encodings\n",
    "    idx to char -- a dictionary mapping character encodings to characters\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    names = file.readlines()\n",
    "    names = [name.lower() for name in names]\n",
    "    file.close()\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    res = file.read().lower()\n",
    "    chars = sorted(list(set(res))) # + \"* \")))\n",
    "    file.close()\n",
    "    \n",
    "    char_to_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "    idx_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    return names, chars, char_to_idx, idx_to_char\n",
    "\n",
    "file_path = \"./data/dinos.txt\"\n",
    "dinosaur_names, chars, char_to_idx, idx_to_char = load_dinosaur_data(file_path)\n",
    "vocab_size = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dinosaur names to fill out the data tensor X and the label tensor y. X is initialized with indexes of letters in dinosaur names, while the rest of the values of X are 0. y is initialized with one hot encodings of t+1 (or next) characters and the rest, while the rest values are 0. Examples are shuffled and data is split into train and test (90 / 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1382, 50, 27)\n",
      "X_test.shape: (154, 50, 27)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(names, length, vocab_size):\n",
    "    X = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "    y = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "\n",
    "    for i, name in enumerate(dinosaur_names):\n",
    "        cur_seq = []\n",
    "        cur_labels = []\n",
    "        for j in range(min(len(name)-1, length)): \n",
    "            c_prev = name[j]\n",
    "            c = name[j+1]\n",
    "            cur_seq.append(char_to_idx[c_prev])\n",
    "            cur_labels.append(char_to_idx[c])\n",
    "        cur_seq = np.array(cur_seq)\n",
    "        cur_seq = to_categorical(cur_seq, num_classes=vocab_size)\n",
    "        cur_labels = np.array(cur_labels)\n",
    "        cur_labels = to_categorical(cur_labels, num_classes=vocab_size)\n",
    "\n",
    "        X[i, 0:min(len(name)-1, length), :] = cur_seq\n",
    "        y[i, 0:min(len(name)-1, length), :] = cur_labels\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "X, y = preprocess_data(dinosaur_names, length, vocab_size)\n",
    "m = X.shape[0]\n",
    "train_m = int(0.9*m)\n",
    "\n",
    "shuffle_inds = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle_inds)\n",
    "\n",
    "X = X[shuffle_inds, :, :]\n",
    "y = y[shuffle_inds, :, :]\n",
    "\n",
    "X_train = X[0:train_m, :, :]\n",
    "X_test = X[train_m:m, :, :]\n",
    "\n",
    "y_train = y[0:train_m, :, :]\n",
    "y_test = y[train_m:m, :, :]\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to specify our LSTM model. It is a simple model with LSTM layer followed by a Dense layer with softmax output of the vocab_size. I also experimented with other model architectures, including more LSTM layers, wider LSTM layers, more Dense layers, Dropout layers and BatchNormalization layers. However, these models have about the same performance, but take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50, 200)           182400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 27)            5427      \n",
      "=================================================================\n",
      "Total params: 187,827\n",
      "Trainable params: 187,827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(n1, vocab_size):\n",
    "    model = keras.Sequential()    \n",
    "    model.add(LSTM(n1, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(200, vocab_size)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our character level LSTM model using 20 epochs. We'll use X_test, y_test to validate out of sample accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1382 samples, validate on 154 samples\n",
      "Epoch 1/40\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.7161 - accuracy: 0.3715 - val_loss: 0.6684 - val_accuracy: 0.2742\n",
      "Epoch 2/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.6422 - accuracy: 0.3059 - val_loss: 0.6340 - val_accuracy: 0.2694\n",
      "Epoch 3/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.6011 - accuracy: 0.2982 - val_loss: 0.6117 - val_accuracy: 0.8177\n",
      "Epoch 4/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.5609 - accuracy: 0.4457 - val_loss: 0.5387 - val_accuracy: 0.2400\n",
      "Epoch 5/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.5013 - accuracy: 0.5445 - val_loss: 0.4992 - val_accuracy: 0.8048\n",
      "Epoch 6/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4742 - accuracy: 0.6679 - val_loss: 0.4803 - val_accuracy: 0.1288\n",
      "Epoch 7/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4588 - accuracy: 0.5165 - val_loss: 0.4666 - val_accuracy: 0.8535\n",
      "Epoch 8/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4436 - accuracy: 0.5815 - val_loss: 0.4515 - val_accuracy: 0.8275\n",
      "Epoch 9/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4308 - accuracy: 0.6513 - val_loss: 0.4370 - val_accuracy: 0.2469\n",
      "Epoch 10/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4225 - accuracy: 0.5419 - val_loss: 0.4351 - val_accuracy: 0.2560\n",
      "Epoch 11/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4140 - accuracy: 0.5273 - val_loss: 0.4246 - val_accuracy: 0.7713\n",
      "Epoch 12/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4054 - accuracy: 0.5029 - val_loss: 0.4173 - val_accuracy: 0.8496\n",
      "Epoch 13/40\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.3966 - accuracy: 0.5335 - val_loss: 0.4105 - val_accuracy: 0.7882\n",
      "Epoch 14/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3903 - accuracy: 0.4410 - val_loss: 0.4067 - val_accuracy: 0.3204\n",
      "Epoch 15/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3860 - accuracy: 0.4608 - val_loss: 0.4058 - val_accuracy: 0.4939\n",
      "Epoch 16/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3815 - accuracy: 0.3142 - val_loss: 0.3981 - val_accuracy: 0.6760\n",
      "Epoch 17/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3760 - accuracy: 0.4429 - val_loss: 0.3968 - val_accuracy: 0.2143\n",
      "Epoch 18/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3717 - accuracy: 0.3126 - val_loss: 0.3982 - val_accuracy: 0.3391\n",
      "Epoch 19/40\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.3665 - accuracy: 0.2504 - val_loss: 0.3991 - val_accuracy: 0.1901\n",
      "Epoch 20/40\n",
      "1382/1382 [==============================] - 3s 3ms/step - loss: 0.3637 - accuracy: 0.2432 - val_loss: 0.3923 - val_accuracy: 0.2697\n",
      "Epoch 21/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3619 - accuracy: 0.2791 - val_loss: 0.3931 - val_accuracy: 0.2334\n",
      "Epoch 22/40\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.3555 - accuracy: 0.2573 - val_loss: 0.3904 - val_accuracy: 0.2634\n",
      "Epoch 23/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3518 - accuracy: 0.3466 - val_loss: 0.3965 - val_accuracy: 0.3314\n",
      "Epoch 24/40\n",
      "1382/1382 [==============================] - 3s 3ms/step - loss: 0.3490 - accuracy: 0.3072 - val_loss: 0.3859 - val_accuracy: 0.6152\n",
      "Epoch 25/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3468 - accuracy: 0.3329 - val_loss: 0.3808 - val_accuracy: 0.3110\n",
      "Epoch 26/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3458 - accuracy: 0.3414 - val_loss: 0.3960 - val_accuracy: 0.3136\n",
      "Epoch 27/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3411 - accuracy: 0.3489 - val_loss: 0.3870 - val_accuracy: 0.2655\n",
      "Epoch 28/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3498 - accuracy: 0.5032 - val_loss: 0.3880 - val_accuracy: 0.4423\n",
      "Epoch 29/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3375 - accuracy: 0.6058 - val_loss: 0.3898 - val_accuracy: 0.5766\n",
      "Epoch 30/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3329 - accuracy: 0.5348 - val_loss: 0.3833 - val_accuracy: 0.7197\n",
      "Epoch 31/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3317 - accuracy: 0.6362 - val_loss: 0.3855 - val_accuracy: 0.6270\n",
      "Epoch 32/40\n",
      "1382/1382 [==============================] - 3s 3ms/step - loss: 0.3266 - accuracy: 0.5316 - val_loss: 0.3907 - val_accuracy: 0.3649\n",
      "Epoch 33/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3211 - accuracy: 0.4126 - val_loss: 0.3874 - val_accuracy: 0.4984\n",
      "Epoch 34/40\n",
      "1382/1382 [==============================] - 3s 3ms/step - loss: 0.3179 - accuracy: 0.4521 - val_loss: 0.3847 - val_accuracy: 0.3404\n",
      "Epoch 35/40\n",
      "1382/1382 [==============================] - 3s 3ms/step - loss: 0.3152 - accuracy: 0.4000 - val_loss: 0.3998 - val_accuracy: 0.2399\n",
      "Epoch 36/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3150 - accuracy: 0.4309 - val_loss: 0.3992 - val_accuracy: 0.3857\n",
      "Epoch 37/40\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3121 - accuracy: 0.3858 - val_loss: 0.4015 - val_accuracy: 0.2622\n",
      "Epoch 38/40\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.3060 - accuracy: 0.3478 - val_loss: 0.3997 - val_accuracy: 0.3479\n",
      "Epoch 39/40\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.3017 - accuracy: 0.3299 - val_loss: 0.3955 - val_accuracy: 0.3617\n",
      "Epoch 40/40\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.2972 - accuracy: 0.3135 - val_loss: 0.3964 - val_accuracy: 0.2664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13d40a950>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=40, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sample from our model one letter at a time. We will start with sampling a random letter from the alphabet and update the input vector x with each new letter we will generate, continuing to feed x to the model to sample the next letter. Once the new line character is encountered, the sampling procedure is over. At the end we report 10 dinosaur names sampled from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaenosaurus\n",
      "oasclosaurus\n",
      "eycernospelos\n",
      "udogonodinator\n",
      "walensaurus\n",
      "xolosaurus\n",
      "jgacosaurus\n",
      "villeritar\n",
      "hyisalnis\n",
      "qespaspin\n"
     ]
    }
   ],
   "source": [
    "def sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length):\n",
    "    first_char_ind = np.random.randint(1, vocab_size)\n",
    "    first_char = idx_to_char[first_char_ind]\n",
    "    res = \"\" + first_char\n",
    "\n",
    "    x = np.zeros((1, 50, vocab_size))\n",
    "    x[0, 0, char_to_idx[first_char]] = 1\n",
    "\n",
    "    for t in range(length):\n",
    "        pred = model.predict(x)[0, t, :]\n",
    "        char_ind = np.random.choice(vocab_size, p=pred)\n",
    "        char = idx_to_char[char_ind]\n",
    "        x[0, t+1, char_ind] = 1\n",
    "    \n",
    "        if(char == '\\n'):\n",
    "            break\n",
    "        else:\n",
    "            res = res + char\n",
    "\n",
    "    return res\n",
    "\n",
    "for i in range(10):\n",
    "    new_name = sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length)\n",
    "    print(new_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
