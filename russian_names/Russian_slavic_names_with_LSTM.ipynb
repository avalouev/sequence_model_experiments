{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Russian old slavic male names with LSTM\n",
    "\n",
    "In this notebook we will build a character level LSTM that can be used to generate traditional slavic / russian male names. To train the model, I collected the data with old slavic names from various russian websites.\n",
    "\n",
    "To generate new names, we will sample from our LSTM model one letter at a time until the end of word. \n",
    "\n",
    "This notebook consists of the following steps:\n",
    "\n",
    "<ul>\n",
    "<li>Implement load_names() function to load names from the file</li>\n",
    "<li>Implement preprocess_data() function to encode name characters into tensors for training</li>\n",
    "<li>Implement build_model() function to define LSTM model architecture</li>\n",
    "<li>Fit the model using Adam optimizer and cross_entropy loss</li>\n",
    "<li>Implement sample_name_from_model() function to sample new names one letter at a time</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Input, Lambda, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some key parameters. The longest name is less than 50 characters, so let's set the model to 50 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50 # define length of the string to be fed to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the names from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique names: 6816\n"
     ]
    }
   ],
   "source": [
    "def load_names(path):\n",
    "    \"\"\"\n",
    "    Loads and returns names\n",
    "    Arguments:\n",
    "    path -- a string with a path to the file with names\n",
    "    \n",
    "    Returns:\n",
    "    names -- a list of names\n",
    "    chars -- a list with vocabulary of letters\n",
    "    char_to_idx -- a dictionary mapping characters to their integer encodings\n",
    "    idx to char -- a dictionary mapping character encodings to characters\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    names = file.readlines()\n",
    "    names = [name.lower() for name in names]\n",
    "    file.close()\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    res = file.read().lower()\n",
    "    chars = sorted(list(set(res))) # + \"* \")))\n",
    "    file.close()\n",
    "    \n",
    "    char_to_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "    idx_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    names = np.unique(names)\n",
    "\n",
    "    return names, chars, char_to_idx, idx_to_char\n",
    "\n",
    "file_path = \"./data/old_slavic_male_names.txt\"\n",
    "dinosaur_names, chars, char_to_idx, idx_to_char = load_names(file_path)\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print(\"unique names: \" + str(len(dinosaur_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_letter_frequencies = np.zeros((len(chars)))\n",
    "for name in dinosaur_names:\n",
    "    char_idx = char_to_idx[name[0]]\n",
    "    first_letter_frequencies[char_idx] = first_letter_frequencies[char_idx]+1\n",
    "first_letter_frequencies = first_letter_frequencies / np.sum(first_letter_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " 'а': 1,\n",
       " 'б': 2,\n",
       " 'в': 3,\n",
       " 'г': 4,\n",
       " 'д': 5,\n",
       " 'е': 6,\n",
       " 'ж': 7,\n",
       " 'з': 8,\n",
       " 'и': 9,\n",
       " 'й': 10,\n",
       " 'к': 11,\n",
       " 'л': 12,\n",
       " 'м': 13,\n",
       " 'н': 14,\n",
       " 'о': 15,\n",
       " 'п': 16,\n",
       " 'р': 17,\n",
       " 'с': 18,\n",
       " 'т': 19,\n",
       " 'у': 20,\n",
       " 'ф': 21,\n",
       " 'х': 22,\n",
       " 'ц': 23,\n",
       " 'ч': 24,\n",
       " 'ш': 25,\n",
       " 'щ': 26,\n",
       " 'ъ': 27,\n",
       " 'ы': 28,\n",
       " 'ь': 29,\n",
       " 'э': 30,\n",
       " 'ю': 31,\n",
       " 'я': 32,\n",
       " 'ё': 33}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print letters to make sure we don't have unwanted symbol\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use names to fill out the data tensor X and the label tensor y. X is initialized with indexes of letters in names, while the rest of the values of X are 0. y is initialized with one hot encodings of t+1 (or next) characters and the rest, while the rest values are 0. Examples are shuffled and data is split into train and test (90 / 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (6134, 50, 34)\n",
      "X_test.shape: (682, 50, 34)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(names, length, vocab_size):\n",
    "    \"\"\"\n",
    "    Preprocesses names for model training\n",
    "    Arguments:\n",
    "    names -- a list of string representing distinct names\n",
    "    length -- an integer representing a length of the string to be fed to the model\n",
    "    vocab_size -- an integer representing the number of unique letters in the list of names\n",
    "    \n",
    "    Returns:\n",
    "    X -- one hot encodings of characters in names, of shape (1536, 50, 29)\n",
    "    Y -- one hot encoding of \"next\" characters for the sequence model, of shape (1536, 50, 29)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "    y = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "\n",
    "    for i, name in enumerate(dinosaur_names):\n",
    "        cur_seq = []\n",
    "        cur_labels = []\n",
    "        for j in range(min(len(name)-1, length)): \n",
    "            c_prev = name[j]\n",
    "            c = name[j+1]\n",
    "            cur_seq.append(char_to_idx[c_prev])\n",
    "            cur_labels.append(char_to_idx[c])\n",
    "        cur_seq = np.array(cur_seq)\n",
    "        cur_seq = to_categorical(cur_seq, num_classes=vocab_size)\n",
    "        cur_labels = np.array(cur_labels)\n",
    "        cur_labels = to_categorical(cur_labels, num_classes=vocab_size)\n",
    "\n",
    "        X[i, 0:min(len(name)-1, length), :] = cur_seq\n",
    "        y[i, 0:min(len(name)-1, length), :] = cur_labels\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(dinosaur_names, length, vocab_size)\n",
    "m = X.shape[0]\n",
    "train_m = int(0.9*m)\n",
    "\n",
    "# Shuffle examples\n",
    "shuffle_inds = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle_inds)\n",
    "X = X[shuffle_inds, :, :]\n",
    "y = y[shuffle_inds, :, :]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train = X[0:train_m, :, :]\n",
    "X_test = X[train_m:m, :, :]\n",
    "\n",
    "y_train = y[0:train_m, :, :]\n",
    "y_test = y[train_m:m, :, :]\n",
    "\n",
    "# report results\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to specify our LSTM model. It is a simple model with several stacked LSTM layers, followed by a Dense layer with softmax output of the vocab_size. I also experimented with other model architectures, including more LSTM layers, wider LSTM layers, more Dense layers, Dropout layers and BatchNormalization layers. However, these models have about the same performance, but take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_55 (LSTM)               (None, 50, 400)           696000    \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 50, 200)           480800    \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 50, 100)           120400    \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 50, 70)            47880     \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 50, 50)            24200     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 50, 34)            1734      \n",
      "=================================================================\n",
      "Total params: 1,371,014\n",
      "Trainable params: 1,371,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(n1, vocab_size):\n",
    "    \"\"\"\n",
    "    Builds character-level LSTM model using Keras\n",
    "\n",
    "    Arguments:\n",
    "    n1 -- number of units in LSTM layer\n",
    "    vocab_size -- size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    model - LSTM model to be trained\n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential()    \n",
    "    model.add(LSTM(n1, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    model.add(LSTM(200, activation='relu', # kernel_initializer='he_normal',\n",
    "                   #input_shape=(length, vocab_size), \n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(100, activation='relu', # kernel_initializer='he_normal',\n",
    "                   #input_shape=(length, vocab_size), \n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(70, activation='relu', # kernel_initializer='he_normal',\n",
    "                   #input_shape=(length, vocab_size), \n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    #model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize a model, compile and print summary\n",
    "model = build_model(400, vocab_size)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our character level LSTM model using 20 epochs. We'll use X_test, y_test to validate out of sample accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6134 samples, validate on 682 samples\n",
      "Epoch 1/20\n",
      "6134/6134 [==============================] - 90s 15ms/step - loss: 0.3910 - accuracy: 0.6478 - val_loss: 0.3752 - val_accuracy: 0.8948\n",
      "Epoch 2/20\n",
      "6134/6134 [==============================] - 88s 14ms/step - loss: 0.3562 - accuracy: 0.4499 - val_loss: 0.3366 - val_accuracy: 0.8292\n",
      "Epoch 3/20\n",
      "6134/6134 [==============================] - 88s 14ms/step - loss: 0.3068 - accuracy: 0.3497 - val_loss: 0.3001 - val_accuracy: 0.3731\n",
      "Epoch 4/20\n",
      "6134/6134 [==============================] - 88s 14ms/step - loss: 0.2926 - accuracy: 0.5191 - val_loss: 0.2910 - val_accuracy: 0.8894\n",
      "Epoch 5/20\n",
      "6134/6134 [==============================] - 88s 14ms/step - loss: 0.2846 - accuracy: 0.6913 - val_loss: 0.2850 - val_accuracy: 0.5878\n",
      "Epoch 6/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2769 - accuracy: 0.7648 - val_loss: 0.2781 - val_accuracy: 0.7643\n",
      "Epoch 7/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2713 - accuracy: 0.7419 - val_loss: 0.2759 - val_accuracy: 0.9096\n",
      "Epoch 8/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2672 - accuracy: 0.5899 - val_loss: 0.2721 - val_accuracy: 0.7697\n",
      "Epoch 9/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2632 - accuracy: 0.6715 - val_loss: 0.2692 - val_accuracy: 0.7987\n",
      "Epoch 10/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2598 - accuracy: 0.4390 - val_loss: 0.2686 - val_accuracy: 0.8973\n",
      "Epoch 11/20\n",
      "6134/6134 [==============================] - 89s 15ms/step - loss: 0.2560 - accuracy: 0.5588 - val_loss: 0.2655 - val_accuracy: 0.3436\n",
      "Epoch 12/20\n",
      "6134/6134 [==============================] - 89s 15ms/step - loss: 0.2531 - accuracy: 0.4488 - val_loss: 0.2650 - val_accuracy: 0.7593\n",
      "Epoch 13/20\n",
      "6134/6134 [==============================] - 89s 15ms/step - loss: 0.2493 - accuracy: 0.5182 - val_loss: 0.2632 - val_accuracy: 0.7210\n",
      "Epoch 14/20\n",
      "6134/6134 [==============================] - 89s 15ms/step - loss: 0.2457 - accuracy: 0.4857 - val_loss: 0.2626 - val_accuracy: 0.4910\n",
      "Epoch 15/20\n",
      "6134/6134 [==============================] - 89s 15ms/step - loss: 0.2428 - accuracy: 0.4504 - val_loss: 0.2598 - val_accuracy: 0.4129\n",
      "Epoch 16/20\n",
      "6134/6134 [==============================] - 89s 15ms/step - loss: 0.2393 - accuracy: 0.3549 - val_loss: 0.2606 - val_accuracy: 0.2677\n",
      "Epoch 17/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2358 - accuracy: 0.2577 - val_loss: 0.2615 - val_accuracy: 0.8025\n",
      "Epoch 18/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2328 - accuracy: 0.3261 - val_loss: 0.2600 - val_accuracy: 0.1479\n",
      "Epoch 19/20\n",
      "6134/6134 [==============================] - 89s 15ms/step - loss: 0.2302 - accuracy: 0.2580 - val_loss: 0.2600 - val_accuracy: 0.1681\n",
      "Epoch 20/20\n",
      "6134/6134 [==============================] - 89s 14ms/step - loss: 0.2274 - accuracy: 0.2759 - val_loss: 0.2581 - val_accuracy: 0.2411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1826e6cd0>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sample from our model one letter at a time. We will start with sampling a random letter from the alphabet and update the input vector x with each new letter we will generate, continuing to feed x to the model to sample the next letter. Once the new line character is encountered, the sampling procedure is over. At the end we report 30 new names sampled from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length, first_letter_frequencies):\n",
    "    \"\"\"\n",
    "    Samples one letter at a time from the model to produce a new name\n",
    "\n",
    "    Arguments:\n",
    "    model -- trained model that is used for sampling\n",
    "    idx_to_char -- a dictionary encoding index to character encoding\n",
    "    char_to_idx -- a dictionary for character to index encoding\n",
    "    vocab_size -- size of the vocabulary of letters\n",
    "    length -- size of the string to be fed into the model\n",
    "    \n",
    "    Returns:\n",
    "    res -- a string representing a sampled new name\n",
    "    \"\"\"\n",
    "    \n",
    "    #first_char_ind = np.random.randint(1, vocab_size)\n",
    "    first_char_ind = np.random.choice(vocab_size, p=first_letter_frequencies)\n",
    "    first_char = idx_to_char[first_char_ind]\n",
    "    res = \"\" + first_char\n",
    "\n",
    "    x = np.zeros((1, 50, vocab_size))\n",
    "    x[0, 0, char_to_idx[first_char]] = 1\n",
    "\n",
    "    for t in range(length):\n",
    "        pred = model.predict(x)[0, t, :]\n",
    "        char_ind = np.random.choice(vocab_size, p=pred)\n",
    "        char = idx_to_char[char_ind]\n",
    "        x[0, t+1, char_ind] = 1\n",
    "    \n",
    "        if(char == '\\n'):\n",
    "            break\n",
    "        else:\n",
    "            res = res + char\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "агнютий\n",
      "харь\n",
      "душевой\n",
      "тлансига\n",
      "здраговад\n",
      "коррий\n",
      "свип\n",
      "дурян\n",
      "сметимир\n",
      "шерка\n",
      "маша\n",
      "колевач\n",
      "оврешко\n",
      "лишок\n",
      "фынлош\n",
      "трорян\n",
      "годран\n",
      "ветоной\n",
      "орешка\n",
      "явыко\n",
      "радест\n",
      "ових\n",
      "кробослав\n",
      "лулбун\n",
      "пудран\n",
      "дюствир\n",
      "сетласий\n",
      "ахлошка\n",
      "попверко\n",
      "прутбяр\n"
     ]
    }
   ],
   "source": [
    "# Now we can generate new names\n",
    "names_to_print = 30\n",
    "total = 0\n",
    "_continue = True\n",
    "while _continue:\n",
    "    new_name = sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length, first_letter_frequencies)\n",
    "    if(new_name+\"\\n\" not in dinosaur_names):\n",
    "        print(new_name)\n",
    "        total = total + 1\n",
    "    if(total >= names_to_print):\n",
    "        _continue = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
