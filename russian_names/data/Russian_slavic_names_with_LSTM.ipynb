{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinosaur names with LSTM\n",
    "\n",
    "In this notebook we will a character level LSTM that can be used to generate dinosaur names. This exercise was inspired by an excercise from Andrew Ng's course on Sequence models. I also used the same dataset for training the model.\n",
    "\n",
    "After we train the model using 1536 dinosaur names, we will sample new dinosaur names one letter at a time using our LSTM model. This notebook consists of the following steps:\n",
    "\n",
    "<ul>\n",
    "<li>Implement load_dinosaur_data() function to load dinosaur names from the file</li>\n",
    "<li>Implement preprocess_data() function to encode dinosaur name characters into tensors for training</li>\n",
    "<li>Implement build_model() function to define LSTM model architecture</li>\n",
    "<li>Fit the model using Adam optimizer and cross_entropy loss</li>\n",
    "<li>Implement sample_name_from_model() function to sample new names one letter at a time</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Input, Lambda, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some key parameters. The longest dinosaur name is 27 characters, so let's set the model to 50 characters in case the generate sequence is a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50 # define length of the string to be fed to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data with dinosaur names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique names: 6816\n"
     ]
    }
   ],
   "source": [
    "def load_dinosaur_data(path):\n",
    "    \"\"\"\n",
    "    Loads and returns dinosaur names data\n",
    "    Arguments:\n",
    "    path -- a string with a path to the file with dinosaur names\n",
    "    \n",
    "    Returns:\n",
    "    names -- a list of dinosaur names\n",
    "    chars -- a list with vocabulary of letters\n",
    "    char_to_idx -- a dictionary mapping characters to their integer encodings\n",
    "    idx to char -- a dictionary mapping character encodings to characters\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    names = file.readlines()\n",
    "    names = [name.lower() for name in names]\n",
    "    file.close()\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    res = file.read().lower()\n",
    "    chars = sorted(list(set(res))) # + \"* \")))\n",
    "    file.close()\n",
    "    \n",
    "    char_to_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "    idx_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    names = np.unique(names)\n",
    "\n",
    "    return names, chars, char_to_idx, idx_to_char\n",
    "\n",
    "file_path = \"./data/old_slavic_male_names.txt\" #dinos.txt\"\n",
    "dinosaur_names, chars, char_to_idx, idx_to_char = load_dinosaur_data(file_path)\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print(\"unique names: \" + str(len(dinosaur_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_letter_frequencies = np.zeros((len(chars)))\n",
    "for name in dinosaur_names:\n",
    "    char_idx = char_to_idx[name[0]]\n",
    "    first_letter_frequencies[char_idx] = first_letter_frequencies[char_idx]+1\n",
    "first_letter_frequencies = first_letter_frequencies / np.sum(first_letter_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " 'а': 1,\n",
       " 'б': 2,\n",
       " 'в': 3,\n",
       " 'г': 4,\n",
       " 'д': 5,\n",
       " 'е': 6,\n",
       " 'ж': 7,\n",
       " 'з': 8,\n",
       " 'и': 9,\n",
       " 'й': 10,\n",
       " 'к': 11,\n",
       " 'л': 12,\n",
       " 'м': 13,\n",
       " 'н': 14,\n",
       " 'о': 15,\n",
       " 'п': 16,\n",
       " 'р': 17,\n",
       " 'с': 18,\n",
       " 'т': 19,\n",
       " 'у': 20,\n",
       " 'ф': 21,\n",
       " 'х': 22,\n",
       " 'ц': 23,\n",
       " 'ч': 24,\n",
       " 'ш': 25,\n",
       " 'щ': 26,\n",
       " 'ъ': 27,\n",
       " 'ы': 28,\n",
       " 'ь': 29,\n",
       " 'э': 30,\n",
       " 'ю': 31,\n",
       " 'я': 32,\n",
       " 'ё': 33}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dinosaur names to fill out the data tensor X and the label tensor y. X is initialized with indexes of letters in dinosaur names, while the rest of the values of X are 0. y is initialized with one hot encodings of t+1 (or next) characters and the rest, while the rest values are 0. Examples are shuffled and data is split into train and test (90 / 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (6134, 50, 34)\n",
      "X_test.shape: (682, 50, 34)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(names, length, vocab_size):\n",
    "    \"\"\"\n",
    "    Preprocesses names for model training\n",
    "    Arguments:\n",
    "    names -- a list of string representing distinct names\n",
    "    length -- an integer representing a length of the string to be fed to the model\n",
    "    vocab_size -- an integer representing the number of unique letters in the list of names\n",
    "    \n",
    "    Returns:\n",
    "    X -- one hot encodings of characters in names, of shape (1536, 50, 29)\n",
    "    Y -- one hot encoding of \"next\" characters for the sequence model, of shape (1536, 50, 29)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "    y = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "\n",
    "    for i, name in enumerate(dinosaur_names):\n",
    "        cur_seq = []\n",
    "        cur_labels = []\n",
    "        for j in range(min(len(name)-1, length)): \n",
    "            c_prev = name[j]\n",
    "            c = name[j+1]\n",
    "            cur_seq.append(char_to_idx[c_prev])\n",
    "            cur_labels.append(char_to_idx[c])\n",
    "        cur_seq = np.array(cur_seq)\n",
    "        cur_seq = to_categorical(cur_seq, num_classes=vocab_size)\n",
    "        cur_labels = np.array(cur_labels)\n",
    "        cur_labels = to_categorical(cur_labels, num_classes=vocab_size)\n",
    "\n",
    "        X[i, 0:min(len(name)-1, length), :] = cur_seq\n",
    "        y[i, 0:min(len(name)-1, length), :] = cur_labels\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(dinosaur_names, length, vocab_size)\n",
    "m = X.shape[0]\n",
    "train_m = int(0.9*m)\n",
    "\n",
    "# Shuffle examples\n",
    "shuffle_inds = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle_inds)\n",
    "X = X[shuffle_inds, :, :]\n",
    "y = y[shuffle_inds, :, :]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train = X[0:train_m, :, :]\n",
    "X_test = X[train_m:m, :, :]\n",
    "\n",
    "y_train = y[0:train_m, :, :]\n",
    "y_test = y[train_m:m, :, :]\n",
    "\n",
    "# report results\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to specify our LSTM model. It is a simple model with LSTM layer followed by a Dense layer with softmax output of the vocab_size. I also experimented with other model architectures, including more LSTM layers, wider LSTM layers, more Dense layers, Dropout layers and BatchNormalization layers. However, these models have about the same performance, but take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_45 (LSTM)               (None, 50, 400)           696000    \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 50, 200)           480800    \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 50, 100)           120400    \n",
      "_________________________________________________________________\n",
      "lstm_48 (LSTM)               (None, 50, 70)            47880     \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 50, 50)            24200     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 50, 34)            1734      \n",
      "=================================================================\n",
      "Total params: 1,371,014\n",
      "Trainable params: 1,371,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(n1, vocab_size):\n",
    "    \"\"\"\n",
    "    Builds character-level LSTM model using Keras\n",
    "\n",
    "    Arguments:\n",
    "    n1 -- number of units in LSTM layer\n",
    "    vocab_size -- size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    model - LSTM model to be trained\n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential()    \n",
    "    model.add(LSTM(n1, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    model.add(LSTM(200, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    model.add(LSTM(100, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    model.add(LSTM(70, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    #model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize a model, compile and print summary\n",
    "model = build_model(400, vocab_size)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our character level LSTM model using 20 epochs. We'll use X_test, y_test to validate out of sample accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6134 samples, validate on 682 samples\n",
      "Epoch 1/20\n",
      "6134/6134 [==============================] - 86s 14ms/step - loss: 0.3925 - accuracy: 0.6907 - val_loss: 0.3730 - val_accuracy: 0.5743\n",
      "Epoch 2/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.3545 - accuracy: 0.6762 - val_loss: 0.3531 - val_accuracy: 0.8627\n",
      "Epoch 3/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.3159 - accuracy: 0.5639 - val_loss: 0.3039 - val_accuracy: 0.3142\n",
      "Epoch 4/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2950 - accuracy: 0.4028 - val_loss: 0.2935 - val_accuracy: 0.3788\n",
      "Epoch 5/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2875 - accuracy: 0.5101 - val_loss: 0.2876 - val_accuracy: 0.2566\n",
      "Epoch 6/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2818 - accuracy: 0.6720 - val_loss: 0.2826 - val_accuracy: 0.9092\n",
      "Epoch 7/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2768 - accuracy: 0.8778 - val_loss: 0.2806 - val_accuracy: 0.9116\n",
      "Epoch 8/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2717 - accuracy: 0.8592 - val_loss: 0.2778 - val_accuracy: 0.9104\n",
      "Epoch 9/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2673 - accuracy: 0.8604 - val_loss: 0.2752 - val_accuracy: 0.8372\n",
      "Epoch 10/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2630 - accuracy: 0.7815 - val_loss: 0.2720 - val_accuracy: 0.9112\n",
      "Epoch 11/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2589 - accuracy: 0.7693 - val_loss: 0.2698 - val_accuracy: 0.8421\n",
      "Epoch 12/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2557 - accuracy: 0.7748 - val_loss: 0.2670 - val_accuracy: 0.6816\n",
      "Epoch 13/20\n",
      "6134/6134 [==============================] - 86s 14ms/step - loss: 0.2527 - accuracy: 0.7100 - val_loss: 0.2634 - val_accuracy: 0.7883\n",
      "Epoch 14/20\n",
      "6134/6134 [==============================] - 86s 14ms/step - loss: 0.2494 - accuracy: 0.6842 - val_loss: 0.2631 - val_accuracy: 0.6950\n",
      "Epoch 15/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2463 - accuracy: 0.6357 - val_loss: 0.2607 - val_accuracy: 0.6289\n",
      "Epoch 16/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2434 - accuracy: 0.5974 - val_loss: 0.2623 - val_accuracy: 0.6369\n",
      "Epoch 17/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2412 - accuracy: 0.5967 - val_loss: 0.2621 - val_accuracy: 0.6884\n",
      "Epoch 18/20\n",
      "6134/6134 [==============================] - 86s 14ms/step - loss: 0.2376 - accuracy: 0.5531 - val_loss: 0.2598 - val_accuracy: 0.5816\n",
      "Epoch 19/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2347 - accuracy: 0.5033 - val_loss: 0.2596 - val_accuracy: 0.2792\n",
      "Epoch 20/20\n",
      "6134/6134 [==============================] - 85s 14ms/step - loss: 0.2325 - accuracy: 0.4196 - val_loss: 0.2600 - val_accuracy: 0.5399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x167decd50>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sample from our model one letter at a time. We will start with sampling a random letter from the alphabet and update the input vector x with each new letter we will generate, continuing to feed x to the model to sample the next letter. Once the new line character is encountered, the sampling procedure is over. At the end we report 10 dinosaur names sampled from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length, first_letter_frequencies):\n",
    "    \"\"\"\n",
    "    Samples one letter at a time from the model to produce a new name\n",
    "\n",
    "    Arguments:\n",
    "    model -- trained model that is used for sampling\n",
    "    idx_to_char -- a dictionary encoding index to character encoding\n",
    "    char_to_idx -- a dictionary for character to index encoding\n",
    "    vocab_size -- size of the vocabulary of letters\n",
    "    length -- size of the string to be fed into the model\n",
    "    \n",
    "    Returns:\n",
    "    res -- a string representing a sampled new name\n",
    "    \"\"\"\n",
    "    \n",
    "    #first_char_ind = np.random.randint(1, vocab_size)\n",
    "    first_char_ind = np.random.choice(vocab_size, p=first_letter_frequencies)\n",
    "    first_char = idx_to_char[first_char_ind]\n",
    "    res = \"\" + first_char\n",
    "\n",
    "    x = np.zeros((1, 50, vocab_size))\n",
    "    x[0, 0, char_to_idx[first_char]] = 1\n",
    "\n",
    "    for t in range(length):\n",
    "        pred = model.predict(x)[0, t, :]\n",
    "        char_ind = np.random.choice(vocab_size, p=pred)\n",
    "        char = idx_to_char[char_ind]\n",
    "        x[0, t+1, char_ind] = 1\n",
    "    \n",
    "        if(char == '\\n'):\n",
    "            break\n",
    "        else:\n",
    "            res = res + char\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "агнютий\n",
      "харь\n",
      "душевой\n",
      "тлансига\n",
      "здраговад\n",
      "коррий\n",
      "свип\n",
      "дурян\n",
      "сметимир\n",
      "шерка\n",
      "маша\n",
      "колевач\n",
      "оврешко\n",
      "лишок\n",
      "фынлош\n",
      "трорян\n",
      "годран\n",
      "ветоной\n",
      "орешка\n",
      "явыко\n",
      "радест\n",
      "ових\n",
      "кробослав\n",
      "лулбун\n",
      "пудран\n",
      "дюствир\n",
      "сетласий\n",
      "ахлошка\n",
      "попверко\n",
      "прутбяр\n"
     ]
    }
   ],
   "source": [
    "# Now we can generate new dinosaur names\n",
    "names_to_print = 30\n",
    "total = 0\n",
    "_continue = True\n",
    "while _continue:\n",
    "    new_name = sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length, first_letter_frequencies)\n",
    "    if(new_name+\"\\n\" not in dinosaur_names):\n",
    "        print(new_name)\n",
    "        total = total + 1\n",
    "    if(total >= names_to_print):\n",
    "        _continue = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
