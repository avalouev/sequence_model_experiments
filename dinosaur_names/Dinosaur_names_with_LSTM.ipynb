{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinosaur names with LSTM\n",
    "\n",
    "In this notebook we will a character level LSTM that can be used to generate dinosaur names. This exercise was inspired by an excercise from Andrew Ng's course on Sequence models. I also used the same dataset for training the model.\n",
    "\n",
    "After we train the model using 1536 dinosaur names, we will sample new dinosaur names one letter at a time using our LSTM model. This notebook consists of the following steps:\n",
    "\n",
    "<ul>\n",
    "<li>Implement load_dinosaur_data() function to load dinosaur names from the file</li>\n",
    "<li>Implement preprocess_data() function to encode dinosaur name characters into tensors for training</li>\n",
    "<li>Implement build_model() function to define LSTM model architecture</li>\n",
    "<li>Fit the model using Adam optimizer and cross_entropy loss</li>\n",
    "<li>Implement sample_name_from_model() function to sample new names one letter at a time</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Input, Lambda, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some key parameters. The longest dinosaur name is 27 characters, so let's set the model to 50 characters in case the generate sequence is a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50 # define length of the string to be fed to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data with dinosaur names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dinosaur_data(path):\n",
    "    \"\"\"\n",
    "    Loads and returns dinosaur names data\n",
    "    Arguments:\n",
    "    path -- a string with a path to the file with dinosaur names\n",
    "    \n",
    "    Returns:\n",
    "    names -- a list of dinosaur names\n",
    "    chars -- a list with vocabulary of letters\n",
    "    char_to_idx -- a dictionary mapping characters to their integer encodings\n",
    "    idx to char -- a dictionary mapping character encodings to characters\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    names = file.readlines()\n",
    "    names = [name.lower() for name in names]\n",
    "    file.close()\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    res = file.read().lower()\n",
    "    chars = sorted(list(set(res))) # + \"* \")))\n",
    "    file.close()\n",
    "    \n",
    "    char_to_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "    idx_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    return names, chars, char_to_idx, idx_to_char\n",
    "\n",
    "file_path = \"./data/dinos.txt\"\n",
    "dinosaur_names, chars, char_to_idx, idx_to_char = load_dinosaur_data(file_path)\n",
    "vocab_size = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dinosaur names to fill out the data tensor X and the label tensor y. X is initialized with indexes of letters in dinosaur names, while the rest of the values of X are 0. y is initialized with one hot encodings of t+1 (or next) characters and the rest, while the rest values are 0. Examples are shuffled and data is split into train and test (90 / 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1382, 50, 27)\n",
      "X_test.shape: (154, 50, 27)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(names, length, vocab_size):\n",
    "    \"\"\"\n",
    "    Preprocesses names for model training\n",
    "    Arguments:\n",
    "    names -- a list of string representing distinct names\n",
    "    length -- an integer representing a length of the string to be fed to the model\n",
    "    vocab_size -- an integer representing the number of unique letters in the list of names\n",
    "    \n",
    "    Returns:\n",
    "    X -- one hot encodings of characters in names, of shape (1536, 50, 29)\n",
    "    Y -- one hot encoding of \"next\" characters for the sequence model, of shape (1536, 50, 29)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "    y = np.zeros((len(dinosaur_names), length, vocab_size))\n",
    "\n",
    "    for i, name in enumerate(dinosaur_names):\n",
    "        cur_seq = []\n",
    "        cur_labels = []\n",
    "        for j in range(min(len(name)-1, length)): \n",
    "            c_prev = name[j]\n",
    "            c = name[j+1]\n",
    "            cur_seq.append(char_to_idx[c_prev])\n",
    "            cur_labels.append(char_to_idx[c])\n",
    "        cur_seq = np.array(cur_seq)\n",
    "        cur_seq = to_categorical(cur_seq, num_classes=vocab_size)\n",
    "        cur_labels = np.array(cur_labels)\n",
    "        cur_labels = to_categorical(cur_labels, num_classes=vocab_size)\n",
    "\n",
    "        X[i, 0:min(len(name)-1, length), :] = cur_seq\n",
    "        y[i, 0:min(len(name)-1, length), :] = cur_labels\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(dinosaur_names, length, vocab_size)\n",
    "m = X.shape[0]\n",
    "train_m = int(0.9*m)\n",
    "\n",
    "# Shuffle examples\n",
    "shuffle_inds = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle_inds)\n",
    "X = X[shuffle_inds, :, :]\n",
    "y = y[shuffle_inds, :, :]\n",
    "\n",
    "# Split in train and test\n",
    "X_train = X[0:train_m, :, :]\n",
    "X_test = X[train_m:m, :, :]\n",
    "\n",
    "y_train = y[0:train_m, :, :]\n",
    "y_test = y[train_m:m, :, :]\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to specify our LSTM model. It is a simple model with LSTM layer followed by a Dense layer with softmax output of the vocab_size. I also experimented with other model architectures, including more LSTM layers, wider LSTM layers, more Dense layers, Dropout layers and BatchNormalization layers. However, these models have about the same performance, but take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50, 200)           182400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 27)            5427      \n",
      "=================================================================\n",
      "Total params: 187,827\n",
      "Trainable params: 187,827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(n1, vocab_size):\n",
    "    \"\"\"\n",
    "    Builds character-level LSTM model using Keras\n",
    "\n",
    "    Arguments:\n",
    "    n1 -- number of units in LSTM layer\n",
    "    vocab_size -- size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    model - LSTM model to be trained\n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential()    \n",
    "    model.add(LSTM(n1, activation='relu', # kernel_initializer='he_normal',\n",
    "                   input_shape=(length, vocab_size), return_sequences=True))\n",
    "    model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize a model, compile and print summary\n",
    "model = build_model(200, vocab_size)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our character level LSTM model using 20 epochs. We'll use X_test, y_test to validate out of sample accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1382 samples, validate on 154 samples\n",
      "Epoch 1/20\n",
      "1382/1382 [==============================] - 4s 3ms/step - loss: 0.7161 - accuracy: 0.3993 - val_loss: 0.6687 - val_accuracy: 0.1681\n",
      "Epoch 2/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.6404 - accuracy: 0.4118 - val_loss: 0.6317 - val_accuracy: 0.5871\n",
      "Epoch 3/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.5943 - accuracy: 0.5084 - val_loss: 0.5783 - val_accuracy: 0.1974\n",
      "Epoch 4/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.5302 - accuracy: 0.2944 - val_loss: 0.5264 - val_accuracy: 0.7082\n",
      "Epoch 5/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4879 - accuracy: 0.6784 - val_loss: 0.4887 - val_accuracy: 0.6442\n",
      "Epoch 6/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4651 - accuracy: 0.6389 - val_loss: 0.4724 - val_accuracy: 0.7796\n",
      "Epoch 7/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4529 - accuracy: 0.7440 - val_loss: 0.4592 - val_accuracy: 0.8562\n",
      "Epoch 8/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4389 - accuracy: 0.7392 - val_loss: 0.4458 - val_accuracy: 0.3869\n",
      "Epoch 9/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4259 - accuracy: 0.6734 - val_loss: 0.4324 - val_accuracy: 0.4801\n",
      "Epoch 10/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4167 - accuracy: 0.7012 - val_loss: 0.4286 - val_accuracy: 0.2371\n",
      "Epoch 11/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4085 - accuracy: 0.4729 - val_loss: 0.4191 - val_accuracy: 0.7655\n",
      "Epoch 12/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.4014 - accuracy: 0.7462 - val_loss: 0.4141 - val_accuracy: 0.7591\n",
      "Epoch 13/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3936 - accuracy: 0.5519 - val_loss: 0.4093 - val_accuracy: 0.7958\n",
      "Epoch 14/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3872 - accuracy: 0.7133 - val_loss: 0.4046 - val_accuracy: 0.7505\n",
      "Epoch 15/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3836 - accuracy: 0.6155 - val_loss: 0.4033 - val_accuracy: 0.4725\n",
      "Epoch 16/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3794 - accuracy: 0.5443 - val_loss: 0.3970 - val_accuracy: 0.4327\n",
      "Epoch 17/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3749 - accuracy: 0.5321 - val_loss: 0.3946 - val_accuracy: 0.3718\n",
      "Epoch 18/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3707 - accuracy: 0.4420 - val_loss: 0.3990 - val_accuracy: 0.5261\n",
      "Epoch 19/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3652 - accuracy: 0.3945 - val_loss: 0.3942 - val_accuracy: 0.3314\n",
      "Epoch 20/20\n",
      "1382/1382 [==============================] - 3s 2ms/step - loss: 0.3621 - accuracy: 0.3334 - val_loss: 0.3911 - val_accuracy: 0.4049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x149ba2d90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sample from our model one letter at a time. We will start with sampling a random letter from the alphabet and update the input vector x with each new letter we will generate, continuing to feed x to the model to sample the next letter. Once the new line character is encountered, the sampling procedure is over. At the end we report 10 dinosaur names sampled from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ijinymangosaurus\n",
      "dabanrtiops\n",
      "ifonentosaurus\n",
      "kacbelomceus\n",
      "fginnsaurus\n",
      "ontrocoraurus\n",
      "ldaxosaurus\n",
      "uavita\n",
      "pabrorosaurus\n",
      "dpaphadon\n"
     ]
    }
   ],
   "source": [
    "def sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length):\n",
    "    \"\"\"\n",
    "    Samples one letter at a time from the model to produce a new name\n",
    "\n",
    "    Arguments:\n",
    "    model -- trained model that is used for sampling\n",
    "    idx_to_char -- a dictionary encoding index to character encoding\n",
    "    char_to_idx -- a dictionary for character to index encoding\n",
    "    vocab_size -- size of the vocabulary of letters\n",
    "    length -- size of the string to be fed into the model\n",
    "    \n",
    "    Returns:\n",
    "    res -- a string representing a sampled new name\n",
    "    \"\"\"\n",
    "    \n",
    "    first_char_ind = np.random.randint(1, vocab_size)\n",
    "    first_char = idx_to_char[first_char_ind]\n",
    "    res = \"\" + first_char\n",
    "\n",
    "    x = np.zeros((1, 50, vocab_size))\n",
    "    x[0, 0, char_to_idx[first_char]] = 1\n",
    "\n",
    "    for t in range(length):\n",
    "        pred = model.predict(x)[0, t, :]\n",
    "        char_ind = np.random.choice(vocab_size, p=pred)\n",
    "        char = idx_to_char[char_ind]\n",
    "        x[0, t+1, char_ind] = 1\n",
    "    \n",
    "        if(char == '\\n'):\n",
    "            break\n",
    "        else:\n",
    "            res = res + char\n",
    "\n",
    "    return res\n",
    "\n",
    "# Now we can generate 10 new dinosaur names\n",
    "for i in range(10):\n",
    "    new_name = sample_name_from_model(model, idx_to_char, char_to_idx, vocab_size, length)\n",
    "    print(new_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
